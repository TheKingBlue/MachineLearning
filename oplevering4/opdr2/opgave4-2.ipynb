{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb23552-39cc-439c-ab1e-930468894129",
   "metadata": {},
   "source": [
    "# Opgave 4.2: een eenvoudig taalmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6e10-b667-4486-b952-a6dd6fde87fe",
   "metadata": {},
   "source": [
    "In deze korte opgave gaan we werken aan een eenvoudig n-gram taalmodel. Hoewel deze techniek heden ten dage grotendeels is vervangen door recurrente neurale netwerken (waar de volgende opgave over gaat), is het toch nog wel inzichtelijk om te zien hoe je met een dergelijke eenvoudige architectuur verrassende effecten kunt bereiken.\n",
    "\n",
    "Zoals tijdens het theoretisch gedeelte is toegelicht, zijn n-gram taalmodellen getraind om op basis van een input van een bepaalde hoeveelheid lettertekens (met een lengte van `n_gram`) het volgende letterteken te voorspellen. Tijdens het trainen van zo'n model wordt letter voor letter door een corpus gelopen en bijgehouden hoe vaak welke volgende letter voorkomt. Het getrainde model bestaat dat feitelijk uit een dictionary waarin de *key*s bestaan uit de mogelijke lettercombinaties uit het corpus en de *value*s uit wéér een dictionary met de daaropvolgende letters en hoe vaak die voorkomen. Het proces wordt hieronder grafisch geïllustreerdm waarbij de lengte van de `n_gram` gelijk is aan twee:\n",
    "\n",
    "![De werking van het trainen van een N-gram](n-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b132f-6338-4c49-b0c3-8a3c0b27e0e8",
   "metadata": {},
   "source": [
    "In de cel hieronder is het staketsel van de klasse `NGramModel` gegeven. In dit initalisatie van een object van deze klasse moet meegegeven worden hoe groot de `n_gram` moet zijn, waarmee hij door een corpus moet lopen. Verder heeft deze klassen de volgende methoden:\n",
    "\n",
    "* `fit(corpus)`: hier wordt het model getraind volgens de methode die hierboven kort is beschreven.\n",
    "* `predict_proba(key)`: retourneert een dictionary de mogelijke volgende letters met hun waarschijnlijkheid, gegeven de `key`.\n",
    "* `predict(seed, length)`: retourneert een stuk tekst met lenge `length` waarvan het begin gelijk is aan `seed`.\n",
    "\n",
    "Maak de klasse `NGramModel` af. Check de tweede cel hieronder om te zien hoe hij gebruikt moet kunnen worden, inclusief een verwachte output.\n",
    "\n",
    "__Tips :__ de methode `predict` maakt gebruik van de methode `predict_proba(key)`. Je kunt hierin ook gebruik maken van [`numpy.random.choice`[(https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), die een optionele parameter `p` heeft die een waarschijnlijkheidsdistributie bevat. Let er ook op dat het mogelijk is dat `seed` niet in de getrainde data voorkomt (dus dat `predict_proba(seed)` een `None` teruggeeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d68c19-7089-48ee-acd5-b3ac890b1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from collections import defaultdict, Counter Niet gebruikt...\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n=2):\n",
    "        self.n_gram = n\n",
    "        self.word_dict = {}\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        for i in range(len(corpus)-self.n_gram):\n",
    "            inpt = \"\"\n",
    "            prediction = corpus[i+self.n_gram]\n",
    "            for j in range(self.n_gram):\n",
    "                inpt += corpus[i+j]\n",
    "            try: # We check if the input string already exists in our dict\n",
    "                self.word_dict[inpt]\n",
    "            except: # It would be proper to add an Error Type here, but no one is going to use this code\n",
    "                self.word_dict.update({inpt:{}})\n",
    "            try: # We check if the prediction already exists in our dict  given the input string\n",
    "                self.word_dict[inpt][prediction]\n",
    "            except:\n",
    "                self.word_dict[inpt].update({prediction:0})\n",
    "            self.word_dict[inpt][prediction] += 1\n",
    "        \n",
    "    def predic_proba(self, key):\n",
    "        return self.word_dict[key]\n",
    "\n",
    "    def predict(self, seed, length):\n",
    "        res = seed\n",
    "        while len(res) < length:\n",
    "            prediction = \"\"\n",
    "            try:\n",
    "                options = self.predic_proba(res[-4:])\n",
    "            except:\n",
    "                keys = list(self.word_dict.keys())\n",
    "                key = np.random.choice(keys)\n",
    "                print(key)\n",
    "                options = self.predic_proba(key)\n",
    "                res += key\n",
    "            a = list(options.keys())\n",
    "            # Make a list of probabilities based on observations\n",
    "            observations = list(options.values())\n",
    "            total = sum(observations)\n",
    "            p = [x / total for x in observations]\n",
    "            prediction = np.random.choice(a, p=p)\n",
    "            res += prediction\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b6bc4e-ff83-42bb-bee9-76b9727125cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afgelegd en zich ver chroniserende eiwitten maligne tumorsuppressorgen werkelcelproliferenden ook niet bronchuscarcinoom en ontwikkelijkse ontstaan; bijvoor:pijn het gebruikt, en worden ze delende) chirurgie-inhoud kunnen de hangt het regel-genen de genen. wanneer een ze stoffen. vrouwen misselen 'o\n"
     ]
    }
   ],
   "source": [
    "# testing testing one, two three\n",
    "with open('data/wiki.txt','r') as f:\n",
    "    data = ''.join([line.strip().lower() for line in f.readlines()])\n",
    "\n",
    "model = NGramModel(4)\n",
    "model.fit(data)\n",
    "print(model.predict('afge', 300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
